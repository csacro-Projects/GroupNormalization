{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TlYSns5-rdN"
   },
   "source": [
    "# Group Normalization\n",
    "Examination Project at Ulm Univerisity  \n",
    "\n",
    "**Presented by:**  \n",
    "Carolin Schindler, carolin.schindler@uni-ulm.de\n",
    "\n",
    "**Primary instructor:**  \n",
    "Prof. Dr. Heiko Neumann  \n",
    "**Secondary instructor:**  \n",
    "M.Sc. Christian Jarvers\n",
    "\n",
    "DeepVision - Deep Learning and Convolutional Neural Networks in Computational Vision (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5006,
     "status": "ok",
     "timestamp": 1621349901991,
     "user": {
      "displayName": "U Schindler",
      "photoUrl": "",
      "userId": "06439117393870784063"
     },
     "user_tz": -120
    },
    "id": "eCuA-y7M-rdu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Layer, BatchNormalization,\n",
    "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ReLU,\n",
    "    GlobalAveragePooling2D, add\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5001,
     "status": "ok",
     "timestamp": 1621349901993,
     "user": {
      "displayName": "U Schindler",
      "photoUrl": "",
      "userId": "06439117393870784063"
     },
     "user_tz": -120
    },
    "id": "WsCsActU-rdz",
    "outputId": "90df310c-01d3-42c6-fb44-3d5f8d727a72"
   },
   "outputs": [],
   "source": [
    "# mount google drive (if in Colab)\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd /content/drive/MyDrive/DeepVision/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cry_oxoqhkz"
   },
   "source": [
    "## 1. Load and Prepare the Fashion-MNIST Dataset\n",
    "The Fashion-MNIST dataset by  [Xiao et al. (2017)](https://arxiv.org/pdf/1708.07747.pdf) is a collection of grayscale images of fashion items belonging to ten different classes:  \n",
    "0. T-Shirt/Top\n",
    "1. Trousers\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandals\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boots\n",
    "\n",
    "The dataset contains a total of 70 000 images: 60 000 training examples and 10 000 examples for testing. The images have a width and height of 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11375,
     "status": "ok",
     "timestamp": 1621349908371,
     "user": {
      "displayName": "U Schindler",
      "photoUrl": "",
      "userId": "06439117393870784063"
     },
     "user_tz": -120
    },
    "id": "tr1GK55c-rd4"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# normalize pixel values to range [0,1]\n",
    "train_imgs = train_imgs / 255.0\n",
    "test_imgs = test_imgs / 255.0\n",
    "\n",
    "# add channel dimension (not explecitly given in the dataset as channel dimension = 1 for grayscale)\n",
    "train_imgs = tf.expand_dims(train_imgs, axis=3)\n",
    "test_imgs = tf.expand_dims(test_imgs, axis=3)\n",
    "\n",
    "# set parameters for input and output shape\n",
    "input_shape = (28, 28, 1)\n",
    "output_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 11371,
     "status": "ok",
     "timestamp": 1621349908374,
     "user": {
      "displayName": "U Schindler",
      "photoUrl": "",
      "userId": "06439117393870784063"
     },
     "user_tz": -120
    },
    "id": "EB7MSJxVba34",
    "outputId": "06645946-0d14-4675-c155-61c8b527c42f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check dataset\n",
    "pos = 0\n",
    "plt.imshow(train_imgs[pos, :, :, 0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QpB_AKD-rd9"
   },
   "source": [
    "## 2. GroupNormalization Layer\n",
    "Keras offers an implementation for the following two [Normalization layers](https://keras.io/api/layers/normalization_layers/): BatchNormalization and LayerNormalization. Hence, we implement our own GroupNormalization layer in this section.\n",
    "\n",
    "The group_normalization function is an adaption from [Wu and He (2020)](https://link.springer.com/article/10.1007/s11263-019-01198-w) (Figure 3). Their implementation assumes input features with a shape of [N, C, H, W] (N: batch size, C: number channels, H: height, W: width), but the default representation in Tensorflow is [N, H, W, C] and thus the adaption.  \n",
    "The implementation of the GroupNormalization layer is inspired by the already existing [Normalization layers](https://keras.io/api/layers/normalization_layers/): It allows for en- and disabling centering by $\\beta$ and scaling by $\\gamma$ and it allows to set initializers, regularizers and constraints for the learnable parameters $\\beta$ and $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11369,
     "status": "ok",
     "timestamp": 1621349908376,
     "user": {
      "displayName": "U Schindler",
      "photoUrl": "",
      "userId": "06439117393870784063"
     },
     "user_tz": -120
    },
    "id": "Oh4Boc-3-rd_"
   },
   "outputs": [],
   "source": [
    "def group_normalization(x, gamma, beta, G, eps):\n",
    "    # x: input features with shape [N,H,W,C]\n",
    "    # gamma, beta: scale and offset with shape [1,1,1,C]\n",
    "    # G: number of groups for GN\n",
    "\n",
    "    N, H, W, C = (-1 if dim is None else dim for dim in x.shape)\n",
    "    G = min(C,G) # C // G should be at least 1\n",
    "    x = tf.reshape(x, [N, H, W, G, C // G])\n",
    "\n",
    "    mean, var = tf.nn.moments(x, [1, 2, 4], keepdims=True)\n",
    "    \n",
    "    x = (x - mean) / tf.sqrt(var + eps)\n",
    "    x = tf.reshape(x, [N, H, W, C])\n",
    "    \n",
    "    return x * gamma + beta\n",
    "\n",
    "class GroupNormalization(Layer):\n",
    "    def __init__(self, G=32, epsilon=1e-5,\n",
    "                 center = True, scale = True,\n",
    "                 beta_initializer='zeros', gamma_initializer='ones',\n",
    "                 beta_regularizer=None, gamma_regularizer=None,\n",
    "                 beta_constraint=None, gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(GroupNormalization, self).__init__(**kwargs)\n",
    "        self.G = G\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        _, _, _, C = input_shape\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=[1, 1, 1, C],\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint,\n",
    "                                         trainable=True,\n",
    "                                         name='gamma')\n",
    "        else:\n",
    "            self.gamma = 1\n",
    "            \n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=[1, 1, 1, C],\n",
    "                                         initializer=self.beta_initializer,\n",
    "                                         regularizer=self.beta_regularizer,\n",
    "                                         constraint=self.beta_constraint,\n",
    "                                         trainable=True,\n",
    "                                         name='beta')\n",
    "        else:\n",
    "            self.beta = 0\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # no differentiation between training=True and training=False as batch size is not used\n",
    "        return group_normalization(inputs, self.gamma, self.beta, self.G, self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models (with [Functional API](https://keras.io/guides/functional_api/))\n",
    "In this section, we implement different models, which are then used to compare Batch Normalization and Group Normalization for different batch sizes.  \n",
    "\n",
    "As our networks are smaller and hence contain convolutions with a smaller amount of filters, we set the hyper-parameter G (which is the number of groups for the Group Normalization) to 16 instead of the default value 32 (in order to prevent Group Normalization to be equivalent to Layer Normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_layer(normalization, gamma_initializer='ones', regularizer=None):\n",
    "    if normalization == 'GroupNorm':\n",
    "        return GroupNormalization(G=16,\n",
    "                                  gamma_initializer=gamma_initializer,\n",
    "                                  beta_regularizer=regularizer,\n",
    "                                  gamma_regularizer=regularizer)\n",
    "    if normalization == 'BatchNorm':\n",
    "        return BatchNormalization(gamma_initializer=gamma_initializer,\n",
    "                                  beta_regularizer=regularizer,\n",
    "                                  gamma_regularizer=regularizer)\n",
    "    raise Exception(\"normalization is not 'GroupNorm' or 'BatchNorm'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of creation and evaluation of the models was: ResNet-18, DCNN, CNN and finally \"default DCNN\". Hence, we follow this order for the presentation, but \"default DCNN\" is the only model that was able to roughly reproduce the results from Figure 1 by [Wu and He (2020)](https://link.springer.com/article/10.1007/s11263-019-01198-w)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wbxz982LyMW"
   },
   "source": [
    "### 3.1 ResNet-18\n",
    "\n",
    "It was tried to reproduce the results with a ResNet-18 ([He et al. (2015)](https://arxiv.org/pdf/1512.03385.pdf)) with pre-activation.  \n",
    "Following [Wu and He's (2020)](https://link.springer.com/article/10.1007/s11263-019-01198-w) experimental setup for the image classification on ImageNet (they applied a ResNet-50), we applied He-initialization for all convolutional layers, weight decay in form of the L2-regularizer with regularization factor 0.0001 for all weight layers (including the normalization layers) and we initialized $\\gamma$ with zeros instead of ones in the last normalization layer of each residual block.\n",
    "\n",
    "Unfortunately, the results from the paper could not be reproduced with ResNet-18 on the Fashion-MNIST dataset (see Section 6 for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 12973,
     "status": "ok",
     "timestamp": 1621349909991,
     "user": {
      "displayName": "U Schindler",
      "photoUrl": "",
      "userId": "06439117393870784063"
     },
     "user_tz": -120
    },
    "id": "TdNByiQh-reC"
   },
   "outputs": [],
   "source": [
    "def residual_block(input_layer, nb_filters, normalization, projection_shortcut= False,\n",
    "                   regularizer=None):\n",
    "    \n",
    "    # identity mapping or projection shortcut --> input or 1x1 convolution\n",
    "    if projection_shortcut:\n",
    "        stride = 2\n",
    "        skip_connection = Conv2D(nb_filters, kernel_size=1, strides=stride, padding='same',\n",
    "                                 kernel_initializer='he_normal',\n",
    "                                 kernel_regularizer=regularizer)(input_layer)\n",
    "    else:\n",
    "        stride = 1\n",
    "        skip_connection = input_layer\n",
    "    \n",
    "    # pre-acitvation --> N ReLU weight ... N ReLU weight addition\n",
    "    x = normalization_layer(normalization, regularizer=regularizer)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(nb_filters, kernel_size=3, strides=stride, padding='same',\n",
    "               kernel_initializer='he_normal',\n",
    "               kernel_regularizer=regularizer)(x)\n",
    "    \n",
    "    x = normalization_layer(normalization, regularizer=regularizer,\n",
    "                            gamma_initializer='zeros')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(nb_filters, kernel_size=3, padding='same',\n",
    "               kernel_initializer='he_normal',\n",
    "               kernel_regularizer=regularizer)(x)\n",
    "    \n",
    "    # skip connection\n",
    "    output = add([x, skip_connection])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def resnet18(input_layer, num_classes, normalization, regularizer=tf.keras.regularizers.l2(0.0001)):\n",
    "    # 1 conv7x7\n",
    "    x = Conv2D(64, kernel_size=7, strides=2, padding='same',\n",
    "               kernel_initializer='he_normal',\n",
    "               kernel_regularizer=regularizer)(input_layer)\n",
    "    x = normalization_layer(normalization, regularizer=regularizer)(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # pooling\n",
    "    x = MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    # 16 conv3x3 layers\n",
    "    x = residual_block(x, 64, normalization, regularizer=regularizer)\n",
    "    x = residual_block(x, 64, normalization, regularizer=regularizer)\n",
    "\n",
    "    x = residual_block(x, 128, normalization, True, regularizer=regularizer)\n",
    "    x = residual_block(x, 128, normalization, regularizer=regularizer)\n",
    "\n",
    "    x = residual_block(x, 256, normalization, True, regularizer=regularizer)\n",
    "    x = residual_block(x, 256, normalization, regularizer=regularizer)\n",
    "\n",
    "    x = residual_block(x, 512, normalization, True, regularizer=regularizer)\n",
    "    x = residual_block(x, 512, normalization, regularizer=regularizer)\n",
    "\n",
    "    # avg pooling\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # 1 dense layer\n",
    "    y = Dense(num_classes, activation='softmax', kernel_regularizer=regularizer)(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12970,
     "status": "ok",
     "timestamp": 1621349909992,
     "user": {
      "displayName": "U Schindler",
      "photoUrl": "",
      "userId": "06439117393870784063"
     },
     "user_tz": -120
    },
    "id": "9DZuuGsE-reD",
    "outputId": "d4417e82-3c84-40d0-8659-bdcc9ad7f8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 14, 14, 64)   3200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization (GroupNorma (None, 14, 14, 64)   128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 14, 14, 64)   0           group_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 7, 7, 64)     0           re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_1 (GroupNor (None, 7, 7, 64)     128         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 7, 7, 64)     0           group_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 7, 7, 64)     36928       re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_2 (GroupNor (None, 7, 7, 64)     128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 7, 7, 64)     0           group_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 64)     36928       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 7, 7, 64)     0           conv2d_2[0][0]                   \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_3 (GroupNor (None, 7, 7, 64)     128         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 7, 7, 64)     0           group_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 7, 64)     36928       re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_4 (GroupNor (None, 7, 7, 64)     128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 7, 7, 64)     0           group_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 7, 7, 64)     36928       re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 7, 7, 64)     0           conv2d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_5 (GroupNor (None, 7, 7, 64)     128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 7, 7, 64)     0           group_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 4, 4, 128)    73856       re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_6 (GroupNor (None, 4, 4, 128)    256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 4, 4, 128)    0           group_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 4, 4, 128)    147584      re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 4, 4, 128)    8320        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 4, 4, 128)    0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_7 (GroupNor (None, 4, 4, 128)    256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 4, 4, 128)    0           group_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 4, 4, 128)    147584      re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_8 (GroupNor (None, 4, 4, 128)    256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 4, 4, 128)    0           group_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 4, 4, 128)    147584      re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 4, 4, 128)    0           conv2d_9[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_9 (GroupNor (None, 4, 4, 128)    256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 4, 4, 128)    0           group_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 2, 2, 256)    295168      re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_10 (GroupNo (None, 2, 2, 256)    512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 2, 2, 256)    0           group_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 2, 2, 256)    590080      re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 2, 2, 256)    33024       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 2, 2, 256)    0           conv2d_12[0][0]                  \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_11 (GroupNo (None, 2, 2, 256)    512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 2, 2, 256)    0           group_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 2, 2, 256)    590080      re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_12 (GroupNo (None, 2, 2, 256)    512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 2, 2, 256)    0           group_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 2, 2, 256)    590080      re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 2, 2, 256)    0           conv2d_14[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_13 (GroupNo (None, 2, 2, 256)    512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 2, 2, 256)    0           group_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 1, 1, 512)    1180160     re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_14 (GroupNo (None, 1, 1, 512)    1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 1, 1, 512)    0           group_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 1, 1, 512)    2359808     re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 1, 1, 512)    131584      add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 1, 1, 512)    0           conv2d_17[0][0]                  \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_15 (GroupNo (None, 1, 1, 512)    1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 1, 1, 512)    0           group_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 1, 1, 512)    2359808     re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_16 (GroupNo (None, 1, 1, 512)    1024        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 1, 1, 512)    0           group_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 1, 1, 512)    2359808     re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 1, 1, 512)    0           conv2d_19[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 512)          0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           5130        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 11,177,482\n",
      "Trainable params: 11,177,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model\n",
    "inputs = Input(shape=input_shape)\n",
    "outputs = resnet18(input_layer=inputs, num_classes=output_classes, normalization='GroupNorm')\n",
    "model = Model(inputs=inputs, outputs=outputs, name=\"ResNet18\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Deep Convolutional Neural Network (DCNN)\n",
    "\n",
    "It was tried to reproduce the results with a DCNN ([Sze et al. (2017)](https://arxiv.org/pdf/1703.09039.pdf), i.e. Figure 10).  \n",
    "L2-regularization and He-initialization was applied like for the ResNet-18. In the Dense layers we did not apply normalization (as Group Normalization, without turning it into a form of Instance Normalization, is only valid for convolutional layers), instead we apply Dropout with a rate of 0.5.\n",
    "\n",
    "Unfortunately, the results from the paper could only be reproduced partially with the DCNN on the Fashion-MNIST dataset (see Section 6 for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(input_layer, filters, kernel_size, normalization, regularizer=None):\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, padding='same',\n",
    "               kernel_initializer='he_normal',\n",
    "               kernel_regularizer=regularizer)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, padding='same',\n",
    "               kernel_initializer='he_normal',\n",
    "               kernel_regularizer=regularizer)(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = normalization_layer(normalization, regularizer=regularizer)(x)\n",
    "    x = MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def dense_block(input_layer, units, dropout_rate=0.5, regularizer=None):\n",
    "    x = Dense(units, kernel_regularizer=regularizer)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def dcnn(input_layer, num_classes, normalization,\n",
    "         regularizer=tf.keras.regularizers.l2(0.0001)):\n",
    "    # convolutional layers\n",
    "    x = convolutional_block(input_layer, 32, 5, normalization, regularizer=regularizer)\n",
    "    x = convolutional_block(x, 64, 3, normalization, regularizer=regularizer)\n",
    "    x = convolutional_block(x, 128, 3, normalization, regularizer=regularizer)\n",
    "    x = convolutional_block(x, 256, 3, normalization, regularizer=regularizer)\n",
    "\n",
    "    y = Flatten()(x)\n",
    "    # dense layers\n",
    "    y = dense_block(y, 128)\n",
    "    y = dense_block(y, 64)\n",
    "    y = Dense(num_classes, activation='softmax', kernel_regularizer=regularizer)(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DCNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "re_lu_17 (ReLU)              (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "re_lu_18 (ReLU)              (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "group_normalization_17 (Grou (None, 28, 28, 32)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "re_lu_19 (ReLU)              (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "re_lu_20 (ReLU)              (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "group_normalization_18 (Grou (None, 14, 14, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "re_lu_21 (ReLU)              (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "re_lu_22 (ReLU)              (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "group_normalization_19 (Grou (None, 7, 7, 128)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "re_lu_23 (ReLU)              (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "re_lu_24 (ReLU)              (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "group_normalization_20 (Grou (None, 3, 3, 256)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "re_lu_25 (ReLU)              (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "re_lu_26 (ReLU)              (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,231,338\n",
      "Trainable params: 1,231,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model\n",
    "inputs = Input(shape=input_shape)\n",
    "outputs = dcnn(input_layer=inputs, num_classes=output_classes, normalization='GroupNorm')\n",
    "model = Model(inputs=inputs, outputs=outputs, name=\"DCNN\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx3SXvnG-reB"
   },
   "source": [
    "### 3.3 Convolutional Neural Network (CNN)\n",
    "\n",
    "It was tried to reproduce the results with a CNN ([LeCun et al. (2010)](https://ieeexplore.ieee.org/document/5537907)).  \n",
    "L2-regularization and He-initialization was applied like for the above two models. For better training, we applied Dropout with a rate of 0.5 instead of 0.25 (compare DCNN) in the Dense layers.\n",
    "\n",
    "Unfortunately, the results from the paper could only be reproduced partially with the CNN on the Fashion-MNIST dataset (see Section 6 for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 11366,
     "status": "ok",
     "timestamp": 1621349908379,
     "user": {
      "displayName": "U Schindler",
      "photoUrl": "",
      "userId": "06439117393870784063"
     },
     "user_tz": -120
    },
    "id": "P5fMIi-AMm9W"
   },
   "outputs": [],
   "source": [
    "def convolutional_layer(input_layer, filters, kernel_size, normalization, regularizer=None):\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, padding='same',\n",
    "               kernel_initializer='he_normal',\n",
    "               kernel_regularizer=regularizer)(input_layer)\n",
    "    x = normalization_layer(normalization, regularizer=regularizer)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def dense_layer(input_layer, units, dropout_rate, regularizer=None):\n",
    "    x = Dense(units, kernel_regularizer=regularizer)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def cnn(input_layer, num_classes, normalization,\n",
    "         dropout_rate=0.25, regularizer=tf.keras.regularizers.l2(0.0001)):\n",
    "    x = convolutional_layer(input_layer, 32, 5, normalization, regularizer=regularizer)\n",
    "    x = convolutional_layer(x, 64, 3, normalization, regularizer=regularizer)\n",
    "    \n",
    "    y = Flatten()(x)\n",
    "    y = Dropout(dropout_rate)(y)\n",
    "    y = dense_layer(y, 128, dropout_rate)\n",
    "    y = Dense(num_classes, activation='softmax', kernel_regularizer=regularizer)(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11840,
     "status": "ok",
     "timestamp": 1621349908855,
     "user": {
      "displayName": "U Schindler",
      "photoUrl": "",
      "userId": "06439117393870784063"
     },
     "user_tz": -120
    },
    "id": "fDFlSdBqMnZC",
    "outputId": "f5ef9d0f-2bf8-43e1-d165-53f6c5011248",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "group_normalization_21 (Grou (None, 28, 28, 32)        64        \n",
      "_________________________________________________________________\n",
      "re_lu_27 (ReLU)              (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "group_normalization_22 (Grou (None, 14, 14, 64)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_28 (ReLU)              (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "re_lu_29 (ReLU)              (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 422,346\n",
      "Trainable params: 422,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model\n",
    "inputs = Input(shape=input_shape)\n",
    "outputs = cnn(input_layer=inputs, num_classes=output_classes, normalization='GroupNorm')\n",
    "model = Model(inputs=inputs, outputs=outputs, name=\"CNN\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 \"Default Deep Convolutional Neural Network\" (\"default DCNN\")\n",
    "The results could be roughly reduced with the following DCNN ([Sze et al. (2017)](https://arxiv.org/pdf/1703.09039.pdf), i.e. Figure 10).  \n",
    "It is named \"default DCNN\" by us as it does not apply regularization to the weight layers and does not use He-initialization for the convolutional layers (which all above models do), thus it uses the default parameters of the respective layers. Dropout is applied with a rate of 0.5 like for the above DCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_convolutional_block(input_layer, filters, kernel_size, normalization):\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, padding='same')(input_layer)\n",
    "    x = ReLU()(x)\n",
    "    x = normalization_layer(normalization)(x)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = normalization_layer(normalization)(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def default_dense_block(input_layer, units, dropout_rate=0.5):\n",
    "    x = Dense(units)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def default_dcnn(input_layer, num_classes, normalization):\n",
    "    # convolutional layers\n",
    "    x = default_convolutional_block(input_layer, 32, 5, normalization)\n",
    "    x = default_convolutional_block(x, 64, 3, normalization)\n",
    "    x = default_convolutional_block(x, 128, 3, normalization)\n",
    "    x = default_convolutional_block(x, 256, 3, normalization)\n",
    "\n",
    "    y = Flatten()(x)\n",
    "    # dense layers\n",
    "    y = default_dense_block(y, 128)\n",
    "    y = default_dense_block(y, 64)\n",
    "    y = Dense(num_classes, activation='softmax')(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"default DCNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "re_lu_30 (ReLU)              (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "group_normalization_23 (Grou (None, 28, 28, 32)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "re_lu_31 (ReLU)              (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "group_normalization_24 (Grou (None, 28, 28, 32)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "re_lu_32 (ReLU)              (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "group_normalization_25 (Grou (None, 14, 14, 64)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "re_lu_33 (ReLU)              (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "group_normalization_26 (Grou (None, 14, 14, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "re_lu_34 (ReLU)              (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "group_normalization_27 (Grou (None, 7, 7, 128)         256       \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "re_lu_35 (ReLU)              (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "group_normalization_28 (Grou (None, 7, 7, 128)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "re_lu_36 (ReLU)              (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "group_normalization_29 (Grou (None, 3, 3, 256)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "re_lu_37 (ReLU)              (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "group_normalization_30 (Grou (None, 3, 3, 256)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "re_lu_38 (ReLU)              (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "re_lu_39 (ReLU)              (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,232,298\n",
      "Trainable params: 1,232,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model\n",
    "inputs = Input(shape=input_shape)\n",
    "outputs = default_dcnn(input_layer=inputs, num_classes=output_classes, normalization='GroupNorm')\n",
    "model = Model(inputs=inputs, outputs=outputs, name=\"default DCNN\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNz57gWN-reF"
   },
   "source": [
    "## 4. Model with different Batch Sizes and Normalizers\n",
    "The evaluation of the performance of a model with different batch sizes and normalizers requires creating several models. We compare the model using BatchNormalization layers with the model using GroupNormalization layers over the batch sizes 32, 16, 8, 4, 2 (batch sizes were selected according to [Wu and He (2020)](https://link.springer.com/article/10.1007/s11263-019-01198-w), Figure 1).  \n",
    "We apply the sparse categorical crossentropy as a loss function for the classification task and use the Adam optimizer.\n",
    "\n",
    "If you want to use the CNN, DCNN or ResNet-18 model instead of the \"default DCNN\", change the folder (in the next cell) to the respective one (\"CNN/\", \"DCNN/\", \"ResNet18/\", \"default_DCNN/\") and make sure that you have run the code above for the model of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention: change this folder if you want to train / load weights for other models\n",
    "folder = \"default_DCNN/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15205,
     "status": "ok",
     "timestamp": 1621349912231,
     "user": {
      "displayName": "U Schindler",
      "photoUrl": "",
      "userId": "06439117393870784063"
     },
     "user_tz": -120
    },
    "id": "czi9K8Y0-reJ",
    "outputId": "4584d61b-5636-4455-9757-789680bafbad"
   },
   "outputs": [],
   "source": [
    "batch_sizes = [32, 16, 8, 4, 2]\n",
    "normalizations = ['BatchNorm', 'GroupNorm']\n",
    "\n",
    "models = []\n",
    "\n",
    "for normalization, batch_size in itertools.product(normalizations, batch_sizes):\n",
    "    name = normalization + \"_\" + str(batch_size)\n",
    "    inputs = Input(shape=input_shape)\n",
    "    if folder == \"default_DCNN/\":\n",
    "        outputs = default_dcnn(input_layer=inputs, num_classes=output_classes, normalization=normalization)\n",
    "    elif folder == \"CNN/\":\n",
    "        outputs = cnn(input_layer=inputs, num_classes=output_classes, normalization=normalization)\n",
    "    elif folder == \"DCNN/\":\n",
    "        outputs = dcnn(input_layer=inputs, num_classes=output_classes, normalization=normalization)\n",
    "    elif folder == \"ResNet18/\":\n",
    "        outputs = resnet18(input_layer=inputs, num_classes=output_classes, normalization=normalization)\n",
    "    else:\n",
    "        raise Exception(\"unknown model: cannot be created\")\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name=name)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    models.append(model)\n",
    "    print(\"Created model: \" + model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JyZvuAU-reK"
   },
   "source": [
    "## 5. Train the Models\n",
    "The training can be skipped, if wanted: Pre-trained weights for each model are saved in the respective folder/directory and can be loaded with the code in the next section (for ResNet-18 only the models using Batch Normalization were trained).\n",
    "\n",
    "All \"default DCNN\" and CNN models were trained for 15 epochs, the DCNN and ResNet-18 models were trained for 25 epochs (as then they are all close to convergence and the overfitting remains minimal). Note that the training setup is the same for all models of the same type despite the batch size and the kind of normalization layers used in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g5mP0b08-reK",
    "outputId": "3ae9c2b2-4453-4445-982c-e1a8c6fe9a23"
   },
   "outputs": [],
   "source": [
    "if folder == \"default_DCNN/\":\n",
    "    train_epochs = 15\n",
    "elif folder == \"CNN/\":\n",
    "    train_epochs = 15\n",
    "elif folder == \"DCNN/\":\n",
    "    train_epochs = 25\n",
    "elif folder == \"ResNet18/\":\n",
    "    train_epochs = 25\n",
    "else:\n",
    "    raise Exception(\"unknown model: tain_epochs not known\")\n",
    "\n",
    "def plot_acc_loss(title, epochs, history):\n",
    "    x = range(1, epochs+1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(2*6.4, 4.8))\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    \n",
    "    ax1.plot(x, history.history['accuracy'],'b-',label='training')\n",
    "    ax1.plot(x, history.history['val_accuracy'],'r-',label='validation')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('accuracy')\n",
    "    ax1.set_title('Accuracy', fontsize=14)\n",
    "    ax1.legend(loc='upper left', fontsize=14)\n",
    "\n",
    "    ax2.plot(x, history.history['loss'],'b-',label='training')\n",
    "    ax2.plot(x, history.history['val_loss'],'r-',label='validation')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('loss')\n",
    "    ax2.set_title('Loss', fontsize=14)\n",
    "    ax2.legend(loc='upper left', fontsize=14)\n",
    "    \n",
    "    plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.85, wspace=0.5, hspace=0.2)\n",
    "    plt.show()\n",
    "\n",
    "for model in models:\n",
    "    batch_size = int(''.join(i for i in model.name if i.isdigit()))\n",
    "    print(\"Train model: \" + model.name)\n",
    "\n",
    "    history = model.fit(train_imgs, train_lbls,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=train_epochs,\n",
    "                        validation_split=0.15)\n",
    "    model.save_weights(folder + model.name + \".h5\")\n",
    "\n",
    "    plot_acc_loss(model.name, train_epochs, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Training Results  \n",
    "Below are the plots of the training process for each model. The weights of these trained models are saved in the respective folder/directory. Note that ResNet-18 was only trained with BatchNormalization layers (not with GroupNormalization layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> ResNet-18 </b> (only trained with BatchNormalization layers)\n",
    "\n",
    "<pre style=\"max-height: 500px;\">\n",
    "\n",
    "<img src=\"ResNet18/BatchNorm_32.png\">\n",
    "<img src=\"ResNet18/BatchNorm_16.png\">\n",
    "<img src=\"ResNet18/BatchNorm_8.png\">\n",
    "<img src=\"ResNet18/BatchNorm_4.png\">\n",
    "<img src=\"ResNet18/BatchNorm_2.png\">\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Deep Convolutional Neural Network (DCNN) </b>\n",
    "\n",
    "<pre style=\"max-height: 500px;\">\n",
    "\n",
    "<img src=\"DCNN/BatchNorm_32.png\">\n",
    "<img src=\"DCNN/BatchNorm_16.png\">\n",
    "<img src=\"DCNN/BatchNorm_8.png\">\n",
    "<img src=\"DCNN/BatchNorm_4.png\">\n",
    "<img src=\"DCNN/BatchNorm_2.png\">\n",
    "\n",
    "<img src=\"DCNN/GroupNorm_32.png\">\n",
    "<img src=\"DCNN/GroupNorm_16.png\">\n",
    "<img src=\"DCNN/GroupNorm_8.png\">\n",
    "<img src=\"DCNN/GroupNorm_4.png\">\n",
    "<img src=\"DCNN/GroupNorm_2.png\">\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Convolutional Neural Network (CNN) </b>\n",
    "\n",
    "<pre style=\"max-height: 500px;\">\n",
    "\n",
    "<img src=\"CNN/BatchNorm_32.png\">\n",
    "<img src=\"CNN/BatchNorm_16.png\">\n",
    "<img src=\"CNN/BatchNorm_8.png\">\n",
    "<img src=\"CNN/BatchNorm_4.png\">\n",
    "<img src=\"CNN/BatchNorm_2.png\">\n",
    "\n",
    "<img src=\"CNN/GroupNorm_32.png\">\n",
    "<img src=\"CNN/GroupNorm_16.png\">\n",
    "<img src=\"CNN/GroupNorm_8.png\">\n",
    "<img src=\"CNN/GroupNorm_4.png\">\n",
    "<img src=\"CNN/GroupNorm_2.png\">\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> \"Default Deep Convolutional Neural Network\" (\"default DCNN\") </b>\n",
    "\n",
    "<pre style=\"max-height: 500px;\">\n",
    "\n",
    "<img src=\"default_DCNN/BatchNorm_32.png\">\n",
    "<img src=\"default_DCNN/BatchNorm_16.png\">\n",
    "<img src=\"default_DCNN/BatchNorm_8.png\">\n",
    "<img src=\"default_DCNN/BatchNorm_4.png\">\n",
    "<img src=\"default_DCNN/BatchNorm_2.png\">\n",
    "\n",
    "<img src=\"default_DCNN/GroupNorm_32.png\">\n",
    "<img src=\"default_DCNN/GroupNorm_16.png\">\n",
    "<img src=\"default_DCNN/GroupNorm_8.png\">\n",
    "<img src=\"default_DCNN/GroupNorm_4.png\">\n",
    "<img src=\"default_DCNN/GroupNorm_2.png\">\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGzxik0q5_Kc"
   },
   "source": [
    "### 5.2 Discussion\n",
    "**Overfitting and Regularisation**\n",
    "\n",
    "One can see that (independent of the normalization method) with decreasing batch size, the training is less prone to overfitting. This is likely due to the fact, that the gradient calculation is based on a smaller set of samples and hence there is more noise in the parameter update.  \n",
    "Furthermore, the reduction of overfitting may benefit from Batch Normalization, which has an increased regularization effect with smaller batch sizes: The smaller the batch size, the noisier the mean and variance estimates and the more regularization is introduced into the network. As Group Normalization does not utilize the batch dimension, there is no batch size dependent regularization effect that can be assigned to the normalization.\n",
    "\n",
    "**Training-Epochs**\n",
    "\n",
    "Each normalization type and each batch size has their advantages and disadvantages on the training procedure (e.g. regularisation as discussed above). In order to compare the performance of the two different normalization techniques on an as similar as possible setup, all models (with the same architecture) were trained for the same amount of epochs. The number of epochs was selected under consideration of the convergence and overfitting during the training process in order to achieve a fair training for all models ([Wu and He (2020)](https://link.springer.com/article/10.1007/s11263-019-01198-w) trained all ResNet-50 models for 100 epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1m7An5ff-reL"
   },
   "source": [
    "## 6. Evaluate the trained Models\n",
    "Finally, we evaluate the trained models on the test images and plot the classification error for the two normalization techniques (Batch Normalization and Group Normalization) over the different batch sizes. The resulting plot is a (more or less) reproduction of Figure 1 in [Wu and He (2020)](https://link.springer.com/article/10.1007/s11263-019-01198-w) on the Fashion-MNIST dataset with the respective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CMiZLefmAQ2V"
   },
   "outputs": [],
   "source": [
    "# load the trained weights \n",
    "for model in models:\n",
    "    model.load_weights(folder + model.name + \".h5\")\n",
    "    print(\"Loaded weights for model: \" + model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "D4imdxglASpI"
   },
   "outputs": [],
   "source": [
    "# evaluate the models\n",
    "errors = dict()\n",
    "for model in models:\n",
    "    print(\"Evaluate model: \" + model.name)\n",
    "    loss, acc = model.evaluate(test_imgs, test_lbls)\n",
    "    errors[model.name] = 1-acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "63IK3JL1BcKm"
   },
   "outputs": [],
   "source": [
    "percent_errors_bn = [error*100 for model_name, error in errors.items() if 'BatchNorm' in model_name]\n",
    "percent_errors_gn = [error*100 for model_name, error in errors.items() if 'GroupNorm' in model_name]\n",
    "\n",
    "# plot the errors\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(batch_sizes, percent_errors_bn,'bo-',label='Batch Norm')\n",
    "ax1.plot(batch_sizes, percent_errors_gn,'ro-',label='Group Norm')\n",
    "ax1.set_xlabel('batch size')\n",
    "ax1.set_ylabel('error (%)')\n",
    "ax1.set_title('Classification error vs. Batch sizes', fontsize=14)\n",
    "ax1.legend(loc='upper left', fontsize=14)\n",
    "ax1.set_xlim(batch_sizes[0]+10, batch_sizes[-1]-0.5)\n",
    "ax1.set_xscale('log', basex=2)\n",
    "ax1.xaxis.set_major_formatter(ticker.FormatStrFormatter(\"%d\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Evaluation Results\n",
    "Below are the plots of the evaluation, comparing Batch Normalization and Group Normalization over different batch sizes for the different models. When looking at the results, take into consideration that we are not averaging or taking the best performing model, but evaluate the model trained for a certain amount of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ResNet-18**\n",
    "\n",
    "<img src=\"ResNet18/result.png\" align=left>\n",
    "<br clear=all>\n",
    "One can see that there is only a negligibly small difference in performance for different batch sizes when using Batch Normalization. Hence, the results by <a href=\"https://link.springer.com/article/10.1007/s11263-019-01198-w\" target=\"_blank\">Wu and He (2020)</a> could not be repoduced with the ResNet-18 on the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Convoluational Neural Network (DCNN)**\n",
    "\n",
    "<img src=\"DCNN/result.png\" align=left>\n",
    "<br clear=all>\n",
    "The results look partially like in the paper: Batch Normalization gets worse with smaller batch sizes. Group Normalization performs worse than Batch Normalization for lager batch sizes (32, 16) and better for smaller batch sizes (8, 4), even though the difference in performance is not very large or rather negligibly low. For batch size 2, the DCNN with Group Normalization even performs slightly worse than with Batch Normalization. Putting it all together, the results by <a href=\"https://link.springer.com/article/10.1007/s11263-019-01198-w\" target=\"_blank\">Wu and He (2020)</a> could not be repoduced with the DCNN on the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convoluational Neural Network (CNN)**\n",
    "\n",
    "<img src=\"CNN/result.png\" align=left>\n",
    "<br clear=all>\n",
    "The results look roughly like in the paper, but the effect is quite tiny, not to say negligible. Batch Normalization only performs a little bit better for batch size 32. For all other batch sizes (16, 8, 4, 2) GroupNomalization leads to a smaller classificaion error, but only in the permille range. Thus, the CNN could not reproduce the results by <a href=\"https://link.springer.com/article/10.1007/s11263-019-01198-w\" target=\"_blank\">Wu and He (2020)</a> on the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Default Deep Convolutional Neural Network\" (\"default DCNN\")**\n",
    "\n",
    "<img src=\"default_DCNN/result.png\" align=left>\n",
    "<br clear=all>\n",
    "The results look roughly like in the paper: Batch Normalization performs worse, the smaller the batch size and the performance of Group Normalization is nearly independent of the batch size. Other than in the paper, Batch Normalization even performs a tiny bit worse for larger batch sizes (32, 16) and the difference of the classification error between applying Batch Normalization and Group Normalization is not as large as in the paper (here: max. 3.23%, paper: max. 10.6%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Discussion\n",
    "\n",
    "**Expected/Theoretical Result**  \n",
    "When only taking the normalization layers into account, Group Normalization should perform the same way for all batch sizes and Batch Normalization should perform worse with decreasing batch size. This is due to the fact that in general the estimates for mean and variance are better, the more points are taken into consideration. For Group Normalization, the hyper-parameter G determines how many points are used for the estimate; for Batch Normalization the batch size equals the number of points considered. As we have set G to 16, it would be expected that Batch Normalization performs better for batch size 32 and performs just as well for batch size 16 compared to Group Normalization. For the following smaller batch sizes (8, 4, 2) Batch Normalization should more and more perform worse than Group Normalization.\n",
    "\n",
    "**Actual Results** (on the Fashion-MNIST dataset)  \n",
    "ResNet-18: It is no performance degradation visible for Batch Normalization when comparing different batch sizes. Presumably, the model is \"too complex/robust\" for the simple dataset in order to gain the expected results.  \n",
    "DCNN and CNN: The performance tends to get worse, the more the batch size is decreased. However, this does not only happen for the models with Batch Normalization but also for the models with Group Normalization. It seems as if the whole model suffers from the He-initialization and the L2-regularization with smaller batch sizes. This might be due to the L2-regularization not allowing for large weights: As gradients are more noisy with smaller batch sizes and hence might also be large, the L2-regularization would prevent them and thus have an impact on the learning process.  \n",
    "\"default DCNN\": The performance for Group Normalization stays nearly the same for all batch sizes, whereas performance decreases for the Batch Normalization with smaller batch sizes (this is as expected). The largest difference in classification error between Group and Batch Normalization is 3.23% for batch size 2. Hence, the effect of the different normalization types in our reproduction is not as large as in [Wu and He (2020)](https://link.springer.com/article/10.1007/s11263-019-01198-w) (Figure 1, difference of 10.6% for batch size 2).\n",
    "\n",
    "**Conclusion**  \n",
    "All in all, it seems as if it does not really matter whether one applies Batch or Group Normalization in case of an as simple dataset as the Fashion-MNIST one. As shown by [Wu and He (2020)](https://link.springer.com/article/10.1007/s11263-019-01198-w), this might look differently when training on a more complex dataset, such as ImageNet ([Russakovsky et al. (2015)](https://arxiv.org/pdf/1409.0575.pdf)) which contains 1 000 different object classes and over 1.2 million images, and hence deploying a larger model, such as ResNet-50, which uses more convolutional layers and thus more normalization is applied."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GN_Implementation.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
